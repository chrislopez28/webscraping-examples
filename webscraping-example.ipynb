{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Example in Python\n",
    "\n",
    "In this notebook we'll scrape food pantry data (e.g. Food Pantry name, Address, hours, contact info, etc.) from the website [www.foodpantries.org](https://www.foodpantries.org). This example is based on a scrape done for [Hack for LA's](https://www.hackforla.org) Food Oasis project. \n",
    "\n",
    "**Quick disclaimer**: I'm not an expert in python or webscrapping, so there are likely cleaner, more efficient ways of doing this example. Nonetheless, I hope that this may be useful and/or helpful for some."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things You'll Need to Run This Example\n",
    "We'll use the following Python libraries:\n",
    "* [Requests](https://requests.readthedocs.io/en/master/)\n",
    "* [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "* [Selenium](https://selenium-python.readthedocs.io/)\n",
    "\n",
    "You can install these packages using [pip](https://pypi.org/project/pip/). If you're running the code from a Jupyter Notebook, make sure you that you have these packages installed in the environment used by the notebook.\n",
    "\n",
    "In addition to the packages noted above, this example uses Google Chrome and the Google Chrome Webdriver. The webdriver can be downloaded here: [https://chromedriver.chromium.org/](https://chromedriver.chromium.org/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup \n",
    "from selenium import webdriver\n",
    "\n",
    "import json\n",
    "import collections\n",
    "from datetime import date\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Browse Website and Develop a Plan\n",
    "\n",
    "Before we start coding, we have to get an idea of what we want to get from the website. Some questions are:\n",
    "* What data do we want?\n",
    "* How do we navigate through the website to get the data we want?\n",
    "* How is data structured on the website?\n",
    "* How is data structured in the website's source code?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Parsing Webpage Data\n",
    "\n",
    "We'll use the `requests.get()` function to get the data located at [https://www.foodpantries.org/st/california](https://www.foodpantries.org/st/california). The function takes in the url of a page (as a string) and returns a **Response** object. This **Response** object contains the data at that url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.foodpantries.org/st/california\"\n",
    "\n",
    "# requests.get :: str -> requests.models.Response\n",
    "page = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the webpage's text by accessing the text attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(page.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now use `BeautifulSoup` to parse the package. We will pass the document `page.text` to the **BeautifulSoup** constructor to a create a **BeautifulSoup** object. As its [documention](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#kinds-of-objects) notes, \"BeautifulSoup transforms a complex HTML document into a complex tree of Python objects.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.text, \"lxml\")\n",
    "\n",
    "# print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = soup.find(\"table\", {\"class\", \"table table-striped\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = []\n",
    "\n",
    "for link in links.find_all(\"a\"):\n",
    "    cities.append([link.text, link.get(\"href\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also write this as a list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = [[link.text, link.get(\"href\")] for link in links.find_all(\"a\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only want cities in Los Angeles County, so let's scrape a list of cities in Los Angeles County from Wikipedia. We're assuming that this list is complete, however this assumption, as with any assumption, should be checked. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_la_cities = \"https://en.wikipedia.org/wiki/List_of_cities_in_Los_Angeles_County,_California\"\n",
    "r = requests.get(url_la_cities)\n",
    "la_soup = BeautifulSoup(r.text, \"lxml\")\n",
    "la_tbl = la_soup.find(\"table\", {\"class\", \"wikitable sortable\"})\n",
    "la_city_links = la_tbl.find_all(\"a\")\n",
    "\n",
    "la_city_list = []\n",
    "\n",
    "for city in la_city_links[1:]:\n",
    "    la_city_list.append(city.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we scrape the list, we should compare the cities in `cities` list with the `la_city_list` to find cities in the `cities` list that are not included in `la_city_list`. In this case, we do have a couple cases:\n",
    "\n",
    "* The entry for Inglewood is mispelled. \n",
    "* Furthermore, several neighborhoods in LA County are listed as cities when in fact they are not cities. \n",
    "\n",
    "Knowing this requires some domain knowledge (in this case, knowledge of local geography), which is extremely important in data analysis. We will append these cases to `la_city_list`. Then create a new list called `la_cities`, which contains only the LA County links from `cities`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_city_list = la_city_list + [\"Ingelwood\", \"North Hollywood\", \"Studio City\", \"Sun Valley\", \"Van Nuys\", \"Woodland Hills\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_cities = [city for city in cities if city[0] in la_city_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automating Scraping of a List of URLs\n",
    "\n",
    "So now we have links to all the LA County city pages for California food pantries. The next step is to visit all these city pages and grab the information on the food pantries included in each of these pages. \n",
    "\n",
    "Instead of manually going through each of these pages using requests and beautifulsoup, we can have selenium automate this process for us. \n",
    "\n",
    "We will create an WebDriver object. Before we do this, we create an Options object so that we can add the options we want for our browser. Here we add an argument for the window-size of the browser and the term *headless*. This means that we can run the Chrome browser without the actual UI--i.e. we won't see the browser. If you want to see the browser go through each of the links, you can remove the line `options.add_argument('headless')`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('window-size=800x841')\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pantries = []\n",
    "\n",
    "for city in la_cities:\n",
    "    driver.implicitly_wait(10)\n",
    "    driver.get(city[1])\n",
    "    innerHTML = driver.execute_script(\"return document.body.innerHTML\")\n",
    "    soup = BeautifulSoup(innerHTML, \"lxml\")\n",
    "    entries = soup.find_all(\"script\",{\"type\":\"application/ld+json\"})\n",
    "    for entry in entries:\n",
    "        pantries.append(entry.text.replace(\"\\n\", \" \").replace(\"\\t\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'             {             \"@context\": \"https://schema.org\",             \"@type\": \"LocalBusiness\",             \"address\": {             \"@type\":\"PostalAddress\",             \"streetAddress\":\"4390 Colfax Avenue\",             \"addressLocality\":\"Studio City\",             \"addressRegion\":\"CA\",             \"postalCode\": \"91604\"             },             \"name\": \"North Hollywood Interfaith Food Pantry\"             ,\"image\": \"https://www.foodpantries.org/gallery/no_photo_1.jpg\"             ,\"description\": \"Hours:Monday - Friday9:00am - 1:00pmPLEASE NOTE THAT THE ENTRANCE IS ON THE TROOST SIDE OF THE PARKING LOT ONLYFor more information, please call....\"             ,\"telephone\":\"(818) 980-1657\"             }         '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pantries[81]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pantry_data = []\n",
    "pantry_err = []\n",
    "\n",
    "for pantry in pantries:\n",
    "    try:\n",
    "        pantry_data.append(json.loads(pantry))\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        pantry_err.append(pantry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282 entries parsed. \n",
      "1 entries not parsed.\n"
     ]
    }
   ],
   "source": [
    "print(\"{} entries parsed. \\n{} entries not parsed.\".format(len(pantry_data), len(pantry_err)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(d, parent_key='', sep='_'):\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = parent_key + sep + k if parent_key else k\n",
    "        if isinstance(v, collections.MutableMapping):\n",
    "            items.extend(flatten(v, new_key, sep=sep).items())\n",
    "        elif isinstance(v, list):\n",
    "            for sub_v in v:\n",
    "                items.extend(flatten(sub_v, new_key, sep=sep).items())\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat = []\n",
    "\n",
    "for pantry in pantry_data:\n",
    "    flat.append(flatten(pantry))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take a look at flat, we can notice a couple things. There are some entries that do not have a \"name\" key. These are not food pantry entries as they contain no information on a food pantry. Also, there are some entries in which the value associated with the \"name\" key are either \"https://www.foodpantries.org\" or \"FoodPantries.org\". These also are not food pantry entries. \n",
    "\n",
    "We create a new corrected list of dictionaries that does not include these entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected = []\n",
    "\n",
    "for entry in flat:\n",
    "    if \"name\" not in entry.keys():\n",
    "        continue\n",
    "    else:\n",
    "        if entry[\"name\"] == \"https://www.foodpantries.org/\" or entry[\"name\"] == \"FoodPantries.org\":\n",
    "            continue\n",
    "    corrected.append(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the List of Dictionaries as a CSV File\n",
    "\n",
    "We use `DictWriter` from the `csv` library to save our scraped data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = set().union(*(d.keys() for d in corrected))\n",
    "\n",
    "date = date.today()\n",
    "filename = \"load_food_pantries_\" + str(date) + \".csv\"\n",
    "\n",
    "with open(filename, 'w', encoding=\"utf-8\", newline=\"\") as output_file:\n",
    "    dict_writer = csv.DictWriter(output_file, keys)\n",
    "    dict_writer.writeheader()\n",
    "    dict_writer.writerows(corrected)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
