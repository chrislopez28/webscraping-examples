{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Example in Python\n",
    "\n",
    "In this notebook we'll scrape food pantry data (e.g. Food Pantry name, Address, hours, contact info, etc.) from the website [www.foodpantries.org](https://www.foodpantries.org). This example is based on a scrape done for [Hack for LA's](https://www.hackforla.org) Food Oasis project. \n",
    "\n",
    "**Quick disclaimer**: I'm not an expert in python or webscrapping, so there are likely cleaner, more efficient ways of doing this example. Nonetheless, I hope that this may be useful and/or helpful for some."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things You'll Need to Run This Example\n",
    "We'll use the following packages:\n",
    "* [Requests](https://requests.readthedocs.io/en/master/)\n",
    "* [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "* [Selenium](https://selenium-python.readthedocs.io/)\n",
    "\n",
    "You can install these packages using [pip](https://pypi.org/project/pip/). If you're running the code from a Jupyter Notebook, make sure you that you have these packages installed in the environment used by the notebook.\n",
    "\n",
    "In addition to the packages noted above, this example uses Google Chrome and the Google Chrome Webdriver. The webdriver can be downloaded here: [https://chromedriver.chromium.org/](https://chromedriver.chromium.org/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup \n",
    "from selenium import webdriver\n",
    "\n",
    "import json\n",
    "import collections\n",
    "from datetime import date\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Browse Website and Develop a Plan\n",
    "\n",
    "Before we start coding, we have to get an idea of what we want to get from the website. Some questions are:\n",
    "* What data do we want?\n",
    "* How do we navigate through the website to get the data we want?\n",
    "* How is data structured on the website?\n",
    "* How is data structured in the website's source code?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Parsing Webpage Data\n",
    "\n",
    "We'll use the `requests.get()` function to get the data located at [https://www.foodpantries.org/st/california](https://www.foodpantries.org/st/california). The function takes in the url of a page (as a string) and returns a **Response** object. This **Response** object contains the data at that url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.foodpantries.org/st/california\"\n",
    "\n",
    "# requests.get :: str -> requests.models.Response\n",
    "page = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the webpage's text by accessing the text attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(page.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now use `BeautifulSoup` to parse the package. We will pass the document `page.text` to the **BeautifulSoup** constructor to a create a **BeautifulSoup** object. As its [documention](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#kinds-of-objects) notes, \"BeautifulSoup transforms a complex HTML document into a complex tree of Python objects.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.text, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = soup.find(\"table\", {\"class\", \"table table-striped\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = []\n",
    "\n",
    "for link in links.find_all(\"a\"):\n",
    "    cities.append([link.text, link.get(\"href\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also write this as a list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = [[link.text, link.get(\"href\")] for link in links.find_all(\"a\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automating Scraping of a List of URLs\n",
    "\n",
    "So now we have links to all the city pages for California food pantries. The next step is to visit all these city pages and grab the information on the food pantries included in each of these pages. \n",
    "\n",
    "Instead of manually going through each of these pages using requests and beautifulsoup, we can have selenium automate this process for us. \n",
    "\n",
    "We will create an WebDriver object. Before we do this, we create an Options object so that we can add the options we want for our browser. Here we add an argument for the window-size of the browser and the term *headless*. This means that we can run the Chrome browser without the actual UI--i.e. we won't see the browser. If you want to see the browser go through each of the links, you can remove the line `options.add_argument('headless')`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('window-size=800x841')\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pantries = []\n",
    "\n",
    "for city in cities:\n",
    "    driver.implicitly_wait(10)\n",
    "    driver.get(city[1])\n",
    "    innerHTML = driver.execute_script(\"return document.body.innerHTML\")\n",
    "    soup = BeautifulSoup(innerHTML, \"lxml\")\n",
    "    entries = soup.find_all(\"script\",{\"type\":\"application/ld+json\"})\n",
    "    for entry in entries:\n",
    "        pantries.append(entry.text.replace(\"\\n\", \" \").replace(\"\\t\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "pantry_data = []\n",
    "pantry_err = []\n",
    "\n",
    "for pantry in pantries:\n",
    "    try:\n",
    "        pantry_data.append(json.loads(pantry))\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        pantry_err.append(pantry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1841 entries parsed. \n",
      "4 entries not parsed.\n"
     ]
    }
   ],
   "source": [
    "print(\"{} entries parsed. \\n{} entries not parsed.\".format(len(pantry_data), len(pantry_err)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(d, parent_key='', sep='_'):\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = parent_key + sep + k if parent_key else k\n",
    "        if isinstance(v, collections.MutableMapping):\n",
    "            items.extend(flatten(v, new_key, sep=sep).items())\n",
    "        elif isinstance(v, list):\n",
    "            for sub_v in v:\n",
    "                items.extend(flatten(sub_v, new_key, sep=sep).items())\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat = []\n",
    "\n",
    "for pantry in pantry_data:\n",
    "    flat.append(flatten(pantry))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take a look at flat, we can notice a couple things. There are some entries that do not have a \"name\" key. These are not food pantry entries as they contain no information on a food pantry. Also, there are some entries in which the value associated with the \"name\" key are either \"https://www.foodpantries.org\" or \"FoodPantries.org\". These also are not food pantry entries. \n",
    "\n",
    "We create a new corrected list of dictionaries that does not include these entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected = []\n",
    "\n",
    "for entry in flat:\n",
    "    if \"name\" not in entry.keys():\n",
    "        continue\n",
    "    else:\n",
    "        if entry[\"name\"] == \"https://www.foodpantries.org/\" or entry[\"name\"] == \"FoodPantries.org\":\n",
    "            continue\n",
    "    corrected.append(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the List of Dictionaries as a CSV File\n",
    "\n",
    "We use `DictWriter` from the `csv` library to save our scraped data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [\"@context\", \"@type\", \"url\", \"sameAs\", \"contactPoint\", \"alternateName\", \n",
    "         \"name\", \"itemListElement\", \"address\", \"image\", \"description\", \"telephone\",\n",
    "        \"contactPoint_@type\", \"contactPoint_contactType\", \"contactPoint_email\",\n",
    "        \"contactPoint_url\", \"itemListElement_item_name\", \"itemListElement_position\",\n",
    "        \"itemListElement_@type\", \"itemListElement_item_@id\", \"address_addressLocality\",\n",
    "        \"address_streetAddress\", \"address_addressRegion\", \"address_postalCode\", \n",
    "        \"address_@type\"]\n",
    "\n",
    "date = date.today()\n",
    "filename = \"foodpantries_scrape_\" + str(date) + \".csv\"\n",
    "\n",
    "with open(filename, 'w', encoding=\"utf-8\", newline=\"\") as output_file:\n",
    "    dict_writer = csv.DictWriter(output_file, keys)\n",
    "    dict_writer.writeheader()\n",
    "    dict_writer.writerows(corrected)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
